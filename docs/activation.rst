.. _activation-functions-label:

Overview of builtin activation functions
========================================

Note that some of these :index:`functions <activation function>` are scaled differently from the canonical
versions you may be familiar with.  The intention of the scaling is to place
more of the functions' "interesting" behavior in the region :math:`\left[-1, 1\right] \times \left[-1, 1\right]`.

The implementation of these functions can be found in the `activations module
<https://github.com/CodeReclaimers/neat-python/blob/master/neat/activations.py>`_.

abs
---

.. figure:: activation-abs.png
   :scale: 50 %
   :alt: absolute value function


clamped
-------

.. figure:: activation-clamped.png
   :scale: 50 %
   :alt: clamped linear function

cube
----

.. figure:: activation-cube.png
   :scale: 50 %
   :alt: cubic function

exp
---

.. figure:: activation-exp.png
   :scale: 50 %
   :alt: exponential function


gauss
-----

.. figure:: activation-gauss.png
   :scale: 50 %
   :alt: gaussian function

hat
---

.. figure:: activation-hat.png
   :scale: 50 %
   :alt: hat function

identity
--------

.. figure:: activation-identity.png
   :scale: 50 %
   :alt: identity function

inv
---

.. figure:: activation-inv.png
   :scale: 50 %
   :alt: inverse function

log
---

.. figure:: activation-log.png
   :scale: 50 %
   :alt: log function

relu
----

.. figure:: activation-relu.png
   :scale: 50 %
   :alt: rectified linear function

sigmoid
-------

.. figure:: activation-sigmoid.png
   :scale: 50 %
   :alt: sigmoid function

sin
---

.. figure:: activation-sin.png
   :scale: 50 %
   :alt: sine function

softplus
--------

.. figure:: activation-softplus.png
   :scale: 50 %
   :alt: soft-plus function

square
------

.. figure:: activation-square.png
   :scale: 50 %
   :alt: square function

tanh
----

.. figure:: activation-tanh.png
   :scale: 50 %
   :alt: hyperbolic tangent function























